{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96e9e60d",
   "metadata": {},
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee4b6c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "171e0b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1612, -0.1066,  0.7112, -2.0103]]) tensor([[ 0.7529, -0.9706, -0.0561,  1.1510]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-3.1245)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dot product of two VECTORS\n",
    "# Dot product = (x)T * (x)\n",
    "X=torch.randn((1,4))\n",
    "Y=torch.randn((1,4))\n",
    "print(X,Y)\n",
    "torch.dot(X[0],Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bc9b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we normalize the two vectors to have a unit length, their dot product tells the cosine of the angle between them.\n",
    "# When weights are positive and sum to 1, then dot product is called weighted average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdcd9814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2197, -0.5776,  0.1494],\n",
      "        [ 0.1942, -0.7180, -1.3790]])\n",
      "tensor([0., 1., 2.])\n",
      "tensor([-0.2787, -3.4760]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Matrix Vector product\n",
    "A=torch.randn((2,3), dtype=torch.float32)\n",
    "x=torch.arange(3, dtype=torch.float32)\n",
    "print(A)\n",
    "print(x)\n",
    "y = torch.mv(A,x)\n",
    "print(y, y.shape)\n",
    "\n",
    "# A=[2,3], x=[3] -----> y=[2]\n",
    "# A useful transformation from 3 to 2 dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff644fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.8165,  0.5862,  2.6685, -2.8223],\n",
       "        [ 0.3464, -0.7907, -0.0780,  1.9494],\n",
       "        [ 2.4999, -1.1593,  3.1194,  0.7055]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix Matrix product\n",
    "A=torch.randn(3,3)\n",
    "B=torch.randn(3,4)\n",
    "torch.mm(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ae2247a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.3852)\n",
      "tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "# Norm refers to magnitude of componenets. Norm is always postive. Euclidean distance is an L2 norm.\n",
    "# The L2 norm of x is the square root of the sum of the squares of the vector elements.\n",
    "# The L1 norm of x is the sum of absolute value of vector elements.\n",
    "a = torch.tensor([2.0,-5.0])\n",
    "print(torch.norm(a)) # L2 Norm\n",
    "print(torch.abs(a).sum()) # L1 Norm\n",
    "# L1 norm is less influenced by errors.\n",
    "# More general form of L1 and L2 norm is LP norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04b6ea27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2320)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frobenius Norm\n",
    "# Frobenius norm of a matrix X âˆˆ R is the square root of the sum of the squares of the matrix elements.\n",
    "# The Frobenius norm satisfies all the properties of vector norms. It behaves as if it were an L2 norm of a matrix-shaped vector\n",
    "torch.norm(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23930ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function are often expressed as norms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b1889",
   "metadata": {},
   "source": [
    "# Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be83c9",
   "metadata": {},
   "source": [
    "Automatic Differentation: Deep learning libraries automatically calculate derivative. System builds a computation graph, tracking which data combined with which operations to produce the output. Automatic differentiation enables the system to subsequently backpropagate gradients. Backpropagate simply means to trace through the computational graph, filling in the partial derivatives with respect to each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32041e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.], requires_grad=True)\n",
      "None\n",
      "tensor(28., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4.0, requires_grad=True)\n",
    "y = 2*torch.dot(x,x)\n",
    "print(x)\n",
    "print(x.grad)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "174434fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can automatically calculate the gradient of y with respect to each component of x by calling the function for \n",
    "# backpropagation and printing the gradient.\n",
    "y.backward()\n",
    "x.grad\n",
    "# Derivative of y is 4x. Then call derivative for each component to get gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54aa9532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch accumulates gradient, so we have to clear the previous gradients.\n",
    "x.grad.zero_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
