{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e18d18b5",
   "metadata": {},
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2196aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "452ed9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1612, -0.1066,  0.7112, -2.0103]]) tensor([[ 0.7529, -0.9706, -0.0561,  1.1510]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-3.1245)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dot product of two VECTORS\n",
    "# Dot product = (x)T * (x)\n",
    "X=torch.randn((1,4))\n",
    "Y=torch.randn((1,4))\n",
    "print(X,Y)\n",
    "torch.dot(X[0],Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2704857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we normalize the two vectors to have a unit length, their dot product tells the cosine of the angle between them.\n",
    "# When weights are positive and sum to 1, then dot product is called weighted average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47d5a3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2197, -0.5776,  0.1494],\n",
      "        [ 0.1942, -0.7180, -1.3790]])\n",
      "tensor([0., 1., 2.])\n",
      "tensor([-0.2787, -3.4760]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Matrix Vector product\n",
    "A=torch.randn((2,3), dtype=torch.float32)\n",
    "x=torch.arange(3, dtype=torch.float32)\n",
    "print(A)\n",
    "print(x)\n",
    "y = torch.mv(A,x)\n",
    "print(y, y.shape)\n",
    "\n",
    "# A=[2,3], x=[3] -----> y=[2]\n",
    "# A useful transformation from 3 to 2 dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6675509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.8165,  0.5862,  2.6685, -2.8223],\n",
       "        [ 0.3464, -0.7907, -0.0780,  1.9494],\n",
       "        [ 2.4999, -1.1593,  3.1194,  0.7055]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix Matrix product\n",
    "A=torch.randn(3,3)\n",
    "B=torch.randn(3,4)\n",
    "torch.mm(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cec18e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.3852)\n",
      "tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "# Norm refers to magnitude of componenets. Norm is always postive. Euclidean distance is an L2 norm.\n",
    "# The L2 norm of x is the square root of the sum of the squares of the vector elements.\n",
    "# The L1 norm of x is the sum of absolute value of vector elements.\n",
    "a = torch.tensor([2.0,-5.0])\n",
    "print(torch.norm(a)) # L2 Norm\n",
    "print(torch.abs(a).sum()) # L1 Norm\n",
    "# L1 norm is less influenced by errors.\n",
    "# More general form of L1 and L2 norm is LP norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f108612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2320)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frobenius Norm\n",
    "# Frobenius norm of a matrix X âˆˆ R is the square root of the sum of the squares of the matrix elements.\n",
    "# The Frobenius norm satisfies all the properties of vector norms. It behaves as if it were an L2 norm of a matrix-shaped vector\n",
    "torch.norm(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b95d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function are often expressed as norms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5255302e",
   "metadata": {},
   "source": [
    "# Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e29774",
   "metadata": {},
   "source": [
    "Automatic Differentation: Deep learning libraries automatically calculate derivative. System builds a computation graph, tracking which data combined with which operations to produce the output. Automatic differentiation enables the system to subsequently backpropagate gradients. Backpropagate simply means to trace through the computational graph, filling in the partial derivatives with respect to each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b3c33a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.], requires_grad=True)\n",
      "None\n",
      "tensor(28., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4.0, requires_grad=True)\n",
    "y = 2*torch.dot(x,x)\n",
    "print(x)\n",
    "print(x.grad)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b7f7ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can automatically calculate the gradient of y with respect to each component of x by calling the function for \n",
    "# backpropagation and printing the gradient.\n",
    "y.backward()\n",
    "x.grad\n",
    "# Derivative of y is 4x. Then call derivative for each component to get gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0dc8bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch accumulates gradient, so we have to clear the previous gradients.\n",
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59489314",
   "metadata": {},
   "source": [
    "For higher-order and higher-dimensional y and x, the differentiation result is a high-order tensor. We calculate\n",
    "the derivatives of the loss functions for each constituent of a batch of training examples. Here, our\n",
    "intent is not to calculate the differentiation matrix but rather the sum of the partial derivatives\n",
    "computed individually for each example in the batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b018863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detaching computation: To move some computations outside of computation graph, we detach it.\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach() # Detaching y\n",
    "z = u * x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
